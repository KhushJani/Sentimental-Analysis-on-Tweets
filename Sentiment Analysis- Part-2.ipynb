{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2475092e",
   "metadata": {},
   "source": [
    "### Load The Tweets Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8cd2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe157846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweets_w/o_SW</th>\n",
       "      <th>cleaned_tweets_with_SW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "      <td>fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone</td>\n",
       "      <td>fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "      <td>finally a transparant silicon case thanks to my uncle yay sony xperia s sonyexperias</td>\n",
       "      <td>finally transparant silicon case thanks uncle yay sony xperia sonyexperias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "      <td>we love this would you go talk makememories unplug relax iphone smartphone wifi connect</td>\n",
       "      <td>love talk makememories unplug relax iphone smartphone wifi connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "      <td>i am wired i know i am george i wa made that way iphone cute daventry home</td>\n",
       "      <td>wired know george way iphone cute daventry home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "      <td>what amazing service apple will not even talk to me about a question i have unless i pay them for their stupid support</td>\n",
       "      <td>amazing service apple talk question unless pay stupid support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "2   3      0   \n",
       "3   4      0   \n",
       "4   5      1   \n",
       "\n",
       "                                                                                                                                 tweet  \\\n",
       "0     #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone   \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/   \n",
       "2          We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu   \n",
       "3                     I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/   \n",
       "4         What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!   \n",
       "\n",
       "                                                                                                     cleaned_tweets_w/o_SW  \\\n",
       "0                         fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone    \n",
       "1                                    finally a transparant silicon case thanks to my uncle yay sony xperia s sonyexperias    \n",
       "2                                 we love this would you go talk makememories unplug relax iphone smartphone wifi connect    \n",
       "3                                              i am wired i know i am george i wa made that way iphone cute daventry home    \n",
       "4  what amazing service apple will not even talk to me about a question i have unless i pay them for their stupid support    \n",
       "\n",
       "                                                                             cleaned_tweets_with_SW  \n",
       "0  fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone   \n",
       "1                       finally transparant silicon case thanks uncle yay sony xperia sonyexperias   \n",
       "2                               love talk makememories unplug relax iphone smartphone wifi connect   \n",
       "3                                                  wired know george way iphone cute daventry home   \n",
       "4                                    amazing service apple talk question unless pay stupid support   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"tweets_cleaned.pkl\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ad7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 refers to positive sentiment, 1 is negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307022d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b20e7286",
   "metadata": {},
   "source": [
    "# 10. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecbefd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r'D:\\OneDrive\\Google Drive Files\\Training\\1 MASTER\\NLP Master\\New Notebooks'\n",
    "# filename = path + r'\\word2vec.txt'\n",
    "#filename = 'word2vec.txt'\n",
    "#model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfbef160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fingerprint',\n",
       " 'pregnancy',\n",
       " 'test',\n",
       " 'android',\n",
       " 'apps',\n",
       " 'beautiful',\n",
       " 'cute',\n",
       " 'health',\n",
       " 'igers',\n",
       " 'iphoneonly',\n",
       " 'iphonesia',\n",
       " 'iphone']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_list = list(data['cleaned_tweets_w/o_SW'].apply(lambda x: x.split()))\n",
    "tweets_list[0] # list of lists, where each tweet is a list of tokens, finally we have a list of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7697eadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\khush\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\khush\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\khush\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\khush\\anaconda3\\lib\\site-packages (from gensim) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e002bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating your own Word2Vec Model & Train\n",
    "from gensim.models import Word2Vec\n",
    "# train model\n",
    "cbow_model = Word2Vec(tweets_list, vector_size = 300, window = 3, min_count=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b6ceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=2420, vector_size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded model\n",
    "print(cbow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ab4625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone',\n",
       " 'apple',\n",
       " 'i',\n",
       " 'my',\n",
       " 'the',\n",
       " 'to',\n",
       " 'a',\n",
       " 'is',\n",
       " 'samsung',\n",
       " 'it',\n",
       " 'and',\n",
       " 'you',\n",
       " 'new',\n",
       " 'twitter',\n",
       " 'for',\n",
       " 'com',\n",
       " 'phone',\n",
       " 'me',\n",
       " 'sony',\n",
       " 'not']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.index_to_key[:20]  # this your vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e8e4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2420"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cbow_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cc3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each document vector will have dimension [1 x 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18780ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a500e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc):\n",
    "    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
    "    \n",
    "    # doc1 contains those words of the document which are included in the vocab\n",
    "    doc1 = [word for word in doc.split() if word in cbow_model.wv.index_to_key]\n",
    "    \n",
    "    wv1 = []  # this will contain the WE of all the vocab words from the doc\n",
    "    for word in doc1:\n",
    "        wv1.append(cbow_model.wv.get_vector(word))\n",
    "    wv1_ = np.array(wv1)\n",
    "    wv1_mean = wv1_.mean(axis=0)\n",
    "    return wv1_mean\n",
    "\n",
    "# np.mean(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86f2c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_temp = data['cleaned_tweets_w/o_SW'].apply(document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d991923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.118848294, 0.26649162, -0.0042150524, 0.07145884, 0.04025153, -0.40189382, 0.1135647, 0.562104, -0.13295817, 0.055012617, -0.009970681, -0.12088235, -0.08324783, -0.077880435, -0.13008295, -0.0...\n",
       "1    [-0.016285757, 0.1781174, -0.025682922, 0.017184386, -0.010458505, -0.3147712, 0.20596102, 0.4821395, -0.016975164, -0.34595996, 0.00207327, -0.32968298, -0.04180927, 0.09435691, -0.24324222, -0.2...\n",
       "2    [-0.026005147, 0.13092043, 0.0710754, 0.11249119, -0.02089282, -0.18087305, 0.208494, 0.4687401, 0.092888676, -0.20213199, 0.047083717, -0.24543484, -0.026217574, 0.044469535, -0.19506095, -0.0797...\n",
       "3    [-0.013127583, 0.12002834, 0.10022379, 0.1694106, -0.03194348, -0.16949905, 0.26162627, 0.5306757, 0.16238648, -0.28069505, 0.07512356, -0.31791216, -0.0062106443, 0.091296956, -0.24322875, -0.060...\n",
       "4    [-0.04881427, 0.106548525, 0.07197899, 0.14075089, -0.040012643, -0.16312374, 0.25407794, 0.49620554, 0.15721549, -0.3134443, 0.06338895, -0.31524593, -0.029927893, 0.07661381, -0.24835801, -0.100...\n",
       "Name: cleaned_tweets_w/o_SW, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_temp[:5]  # displaying the 1st 5 tweets, as document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14007df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_temp[0].shape  # each document vecotr is 300-dimensional !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ad5dc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f66a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16e2f992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining all the document vectors into a singl numpy array (tweets_vec)\n",
    "embedding_size = 300\n",
    "tweets_vec = np.ones((len(tweets_temp), embedding_size))*np.nan\n",
    "for i in range(tweets_vec.shape[0]):\n",
    "    tweets_vec[i,:] = tweets_temp.iloc[i]\n",
    "\n",
    "tweets_vec.shape # this itself is your final FEATURE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7875e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "402b2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DF to store these new documnent features\n",
    "df = pd.DataFrame(tweets_vec)\n",
    "df['y'] = data['label']\n",
    "df.dropna(how='any', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cacd3bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.118848</td>\n",
       "      <td>0.266492</td>\n",
       "      <td>-0.004215</td>\n",
       "      <td>0.071459</td>\n",
       "      <td>0.040252</td>\n",
       "      <td>-0.401894</td>\n",
       "      <td>0.113565</td>\n",
       "      <td>0.562104</td>\n",
       "      <td>-0.132958</td>\n",
       "      <td>0.055013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264827</td>\n",
       "      <td>0.021766</td>\n",
       "      <td>0.051733</td>\n",
       "      <td>0.256626</td>\n",
       "      <td>0.303706</td>\n",
       "      <td>0.061044</td>\n",
       "      <td>-0.158649</td>\n",
       "      <td>0.191490</td>\n",
       "      <td>-0.082921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.016286</td>\n",
       "      <td>0.178117</td>\n",
       "      <td>-0.025683</td>\n",
       "      <td>0.017184</td>\n",
       "      <td>-0.010459</td>\n",
       "      <td>-0.314771</td>\n",
       "      <td>0.205961</td>\n",
       "      <td>0.482139</td>\n",
       "      <td>-0.016975</td>\n",
       "      <td>-0.345960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296096</td>\n",
       "      <td>0.150883</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.322839</td>\n",
       "      <td>0.306283</td>\n",
       "      <td>-0.009773</td>\n",
       "      <td>-0.221175</td>\n",
       "      <td>0.170154</td>\n",
       "      <td>-0.159251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.026005</td>\n",
       "      <td>0.130920</td>\n",
       "      <td>0.071075</td>\n",
       "      <td>0.112491</td>\n",
       "      <td>-0.020893</td>\n",
       "      <td>-0.180873</td>\n",
       "      <td>0.208494</td>\n",
       "      <td>0.468740</td>\n",
       "      <td>0.092889</td>\n",
       "      <td>-0.202132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281722</td>\n",
       "      <td>0.142496</td>\n",
       "      <td>0.075452</td>\n",
       "      <td>0.306799</td>\n",
       "      <td>0.251461</td>\n",
       "      <td>0.042114</td>\n",
       "      <td>-0.088374</td>\n",
       "      <td>0.103579</td>\n",
       "      <td>-0.096197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.013128</td>\n",
       "      <td>0.120028</td>\n",
       "      <td>0.100224</td>\n",
       "      <td>0.169411</td>\n",
       "      <td>-0.031943</td>\n",
       "      <td>-0.169499</td>\n",
       "      <td>0.261626</td>\n",
       "      <td>0.530676</td>\n",
       "      <td>0.162386</td>\n",
       "      <td>-0.280695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346176</td>\n",
       "      <td>0.196281</td>\n",
       "      <td>0.118380</td>\n",
       "      <td>0.413562</td>\n",
       "      <td>0.258920</td>\n",
       "      <td>0.067381</td>\n",
       "      <td>-0.056107</td>\n",
       "      <td>0.098618</td>\n",
       "      <td>-0.099734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.048814</td>\n",
       "      <td>0.106549</td>\n",
       "      <td>0.071979</td>\n",
       "      <td>0.140751</td>\n",
       "      <td>-0.040013</td>\n",
       "      <td>-0.163124</td>\n",
       "      <td>0.254078</td>\n",
       "      <td>0.496206</td>\n",
       "      <td>0.157215</td>\n",
       "      <td>-0.313444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324228</td>\n",
       "      <td>0.197239</td>\n",
       "      <td>0.085207</td>\n",
       "      <td>0.372156</td>\n",
       "      <td>0.274682</td>\n",
       "      <td>0.048920</td>\n",
       "      <td>-0.088192</td>\n",
       "      <td>0.080665</td>\n",
       "      <td>-0.115826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.118848  0.266492 -0.004215  0.071459  0.040252 -0.401894  0.113565   \n",
       "1 -0.016286  0.178117 -0.025683  0.017184 -0.010459 -0.314771  0.205961   \n",
       "2 -0.026005  0.130920  0.071075  0.112491 -0.020893 -0.180873  0.208494   \n",
       "3 -0.013128  0.120028  0.100224  0.169411 -0.031943 -0.169499  0.261626   \n",
       "4 -0.048814  0.106549  0.071979  0.140751 -0.040013 -0.163124  0.254078   \n",
       "\n",
       "          7         8         9  ...       291       292       293       294  \\\n",
       "0  0.562104 -0.132958  0.055013  ...  0.264827  0.021766  0.051733  0.256626   \n",
       "1  0.482139 -0.016975 -0.345960  ...  0.296096  0.150883  0.008503  0.322839   \n",
       "2  0.468740  0.092889 -0.202132  ...  0.281722  0.142496  0.075452  0.306799   \n",
       "3  0.530676  0.162386 -0.280695  ...  0.346176  0.196281  0.118380  0.413562   \n",
       "4  0.496206  0.157215 -0.313444  ...  0.324228  0.197239  0.085207  0.372156   \n",
       "\n",
       "        295       296       297       298       299  y  \n",
       "0  0.303706  0.061044 -0.158649  0.191490 -0.082921  0  \n",
       "1  0.306283 -0.009773 -0.221175  0.170154 -0.159251  0  \n",
       "2  0.251461  0.042114 -0.088374  0.103579 -0.096197  0  \n",
       "3  0.258920  0.067381 -0.056107  0.098618 -0.099734  0  \n",
       "4  0.274682  0.048920 -0.088192  0.080665 -0.115826  1  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d33725d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 301)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b3d5a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_word_emb = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "X_word_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c219b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943f84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10b75e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.76 0.22\n",
      "85.42 0.81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4, random_state=42)\n",
    "WE_pipe = Pipeline([('SC', StandardScaler()), ('LR', LR1)] )\n",
    "\n",
    "results = cross_validate(WE_pipe, X_word_emb, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# print(results['train_score'])\n",
    "print(np.round((results['train_score'].mean())*100, 2), np.round((results['train_score'].std())*100, 2)) \n",
    "\n",
    "# print(results['test_score'])\n",
    "print(np.round((results['test_score'].mean())*100, 2), np.round((results['test_score'].std())*100, 2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17f5c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.95 0.07\n",
      "87.75 1.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['cleaned_tweets_w/o_SW']\n",
    "y = data['label']\n",
    "\n",
    "# we want to include only those words in the vocab which have min df of 5,\n",
    "# means select only those words which occur ATLEAST in 5 documents!! \n",
    "# AND SELECT the TOP 300 FEATURES ONLY to build the model\n",
    "CV = CountVectorizer(min_df=5, max_features=300)\n",
    "\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4)\n",
    "CV_pipe = Pipeline([('CV', CV) , ('LR', LR1)] )\n",
    "results = cross_validate(CV_pipe, X, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# print(results['train_score'])\n",
    "print(np.round((results['train_score'].mean())*100, 2), np.round((results['train_score'].std())*100, 2)) \n",
    "\n",
    "# print(results['test_score'])\n",
    "print(np.round((results['test_score'].mean())*100, 2), np.round((results['test_score'].std())*100, 2)) \n",
    "\n",
    "CV.fit_transform(X)\n",
    "len(CV.vocabulary_)  # no. of features AFTER applying the stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f88458",
   "metadata": {},
   "source": [
    "# 11. Word Embeddings from GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17bbeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00836f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the converted model\n",
    "filename = 'word2vec.txt'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "631749a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.025135, -1.1037  , -0.014392,  0.175   ,  0.45659 , -0.86727 ,\n",
       "       -0.057021, -0.66513 ,  0.35031 ,  0.46178 , -0.079201, -0.15928 ,\n",
       "       -0.29051 , -0.37331 ,  0.58284 ,  0.47992 ,  0.47444 ,  0.018436,\n",
       "        0.33742 ,  0.48474 , -1.0344  , -0.63262 , -0.043848,  0.33803 ,\n",
       "       -0.27473 ,  0.46233 ,  0.92311 ,  1.6516  , -0.99585 , -0.41202 ,\n",
       "       -0.22485 ,  0.17227 , -0.82582 ,  0.046938,  1.0012  , -0.22104 ,\n",
       "       -0.81985 ,  0.072396,  0.67151 , -0.80752 ,  0.2998  , -0.20886 ,\n",
       "       -1.3073  , -0.085651, -1.2405  , -0.59945 , -0.38276 , -0.014263,\n",
       "        0.17119 ,  0.19705 , -0.17824 , -0.11378 ,  0.24159 ,  0.057804,\n",
       "        0.044002, -1.1791  ,  0.48858 , -0.78541 ,  0.06117 ,  0.19021 ,\n",
       "       -0.27743 , -0.9376  , -0.43884 ,  0.10984 , -0.59379 , -0.13567 ,\n",
       "        0.050591, -0.062951,  1.2968  ,  0.35529 , -0.87356 ,  0.61764 ,\n",
       "       -0.23356 , -0.74894 ,  0.35229 , -0.99631 ,  0.33625 , -0.027754,\n",
       "       -0.85467 , -1.1996  ,  0.60355 ,  0.90339 , -0.6848  , -0.76649 ,\n",
       "        0.16188 , -0.12005 ,  0.43946 , -0.30027 ,  0.6662  , -0.48541 ,\n",
       "        0.036112, -0.34172 ,  0.68198 , -0.23978 ,  0.53855 ,  0.16917 ,\n",
       "        0.15406 , -0.19322 ,  0.085335,  1.0393  ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector('analytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22029e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "467a726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector_GloVe(doc):\n",
    "    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
    "    \n",
    "    # doc1 contains those words of the document which are included in the vocab\n",
    "    doc1 = [word for word in doc.split() if word in model.index_to_key]\n",
    "    \n",
    "    wv1 = []  # this will contain the WE of all the vocab words from the doc\n",
    "    for word in doc1:\n",
    "        wv1.append(model.get_vector(word))\n",
    "    wv1_ = np.array(wv1)\n",
    "    wv1_mean = wv1_.mean(axis=0)\n",
    "    return wv1_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7b3fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_temp = data['cleaned_tweets_w/o_SW'].apply(document_vector_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d87f8af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.12796232, 0.004934112, 0.2997002, -0.1567011, -0.15583865, 0.09799757, 0.11052724, 0.035929985, 0.22409555, 0.35114682, 0.115047574, -0.15497755, 0.023922225, -0.14416587, 0.61092, 0.21855089,...\n",
       "1    [0.10285442, -0.03414092, 0.44059834, 0.019384174, 0.0109245, -0.039047826, 0.014194754, -0.07490217, -0.098265, 0.039055835, 0.2747599, 0.06436991, -0.052626252, 0.019468, -0.050155003, -0.234040...\n",
       "2    [-0.22620347, 0.12656459, 0.42639765, -0.27715844, -0.15769385, 0.19056438, -0.24652052, 0.096992, 0.34241086, -0.13856545, 0.19085309, 0.1367206, 0.060512602, -0.19153553, 0.20073484, -0.24162795...\n",
       "3    [-0.15686598, 0.19567013, 0.49366295, -0.3283021, -0.46933356, 0.2035618, -0.02267706, 0.1738718, 0.19721664, -0.04747359, 0.51096183, 0.06516355, 0.1846259, 0.033529352, 0.18381229, -0.37266436, ...\n",
       "4    [-0.07447391, 0.26465738, 0.3381867, -0.2901459, -0.3280814, 0.20322695, -0.18568636, 0.209613, 0.060292058, -0.121835224, 0.14546065, 0.14423917, 0.21310659, -0.08253591, 0.08832547, -0.33873826,...\n",
       "Name: cleaned_tweets_w/o_SW, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_temp[:5]  # displaying the 1st 5 tweets, as document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52ec7fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining all the document vectors into a singl numpy array (tweets_vec)\n",
    "embedding_size = 100\n",
    "tweets_vec = np.ones((len(tweets_temp), embedding_size))*np.nan\n",
    "for i in range(tweets_vec.shape[0]):\n",
    "    tweets_vec[i,:] = tweets_temp.iloc[i]\n",
    "\n",
    "# tweets_vec.shape # this itself is your final FEATURE MATRIX\n",
    "# Create a new DF to store these new documnent features\n",
    "df1 = pd.DataFrame(tweets_vec)\n",
    "df1['y'] = data['label']\n",
    "df1.dropna(how='any', axis=0, inplace=True)\n",
    "\n",
    "X_word_emb = df1.drop('y', axis=1)\n",
    "y = df1['y']\n",
    "X_word_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b09b6058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.22 0.03\n",
      "86.29 0.67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4, random_state=42)\n",
    "WE_pipe = Pipeline([('SC', StandardScaler()), ('LR', LR1)] )\n",
    "\n",
    "results = cross_validate(WE_pipe, X_word_emb, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# print(results['train_score'])\n",
    "print(np.round((results['train_score'].mean())*100, 2), np.round((results['train_score'].std())*100, 2)) \n",
    "\n",
    "# print(results['test_score'])\n",
    "print(np.round((results['test_score'].mean())*100, 2), np.round((results['test_score'].std())*100, 2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9220b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e601473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
